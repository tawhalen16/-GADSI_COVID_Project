{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeoCFhZMw4yr"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHO736Dww4ys"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHOZCJB9w4yx"
   },
   "source": [
    "## List Files to Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfi0UklQw4yx"
   },
   "outputs": [],
   "source": [
    "# Import current list of files and their cleaning status\n",
    "df_combined_data = pd.read_csv('./Data/combined_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9Dx1HXiw4y0"
   },
   "outputs": [],
   "source": [
    "# Import names of all \"Coronavirus Tweets\" files currently in /Data directory\n",
    "# .find method taken from:\n",
    "# https://www.afternerd.com/blog/python-string-contains/\n",
    "data_files = list(filter(lambda file: (file.find('Coronavirus Tweets') != -1), os.listdir('./Data')))\n",
    "\n",
    "# Filter list of files to new files only\n",
    "new_files = list(filter(lambda file: file not in df_combined_data['file'].values , data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvMIcIb_w4y5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create dataframe of new files. Set added status as 0\n",
    "df_new_files = pd.DataFrame([new_files,[0]*len(new_files)], index = ['file','added']).T\n",
    "\n",
    "# Append new files dataframe to existing dataframe\n",
    "df_combined_data = df_combined_data.append(df_new_files).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3CMo8Jmw4y7"
   },
   "source": [
    "## Build Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ997K0Gw4y8"
   },
   "outputs": [],
   "source": [
    "# Create dictionary of US state names and codes from:\n",
    "# https://gist.github.com/rogerallen/1583593\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI',\n",
    "    'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND', 'Northern Mariana Islands':'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA', 'Puerto Rico': 'PR', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA',\n",
    "    'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovlSynfBw4y-"
   },
   "outputs": [],
   "source": [
    "# Define function to filter dataframe to tweets that are:\n",
    "# 1) from the US, 2) with valid locations 3) more granular than country, and 4) in english\n",
    "def get_US_data(df):\n",
    "    mask_us = (df['country_code'] == 'US')\n",
    "    mask_place = (df['place_full_name'].notna())\n",
    "    mask_country = (df['place_type'] != 'country')\n",
    "    mask_eng = (df['lang'] == 'en')\n",
    "    return df.loc[mask_us & mask_place & mask_country & mask_eng,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvuMFAyBw4zB"
   },
   "outputs": [],
   "source": [
    "# Define function to correct data types and drop unnecessary columns\n",
    "def correct_columns_kag(df):\n",
    "    # Set datetime columns as pandas datetime type\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['account_created_at'] = pd.to_datetime(df['account_created_at'])\n",
    "    \n",
    "    # Create boolean column for whether a tweet is a reply and drop all other reply columns\n",
    "    df['is_reply'] = df['reply_to_user_id'].notna()\n",
    "    \n",
    "    # Change spelling of favorites_count to match IEEE\n",
    "    df.rename(columns = {'favourites_count': 'favorite_count'}, inplace = True)\n",
    "                         \n",
    "    # Remove all unneeded columns\n",
    "    df = df.drop(columns = ['account_lang','reply_to_user_id','reply_to_status_id','reply_to_screen_name','source',\n",
    "                            'user_id','is_quote'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to correct names of columns from IEEE dataset\n",
    "def correct_names_ieee(df):\n",
    "    # Rename columns to match original dataset\n",
    "    df.rename(columns = {'user_followers_count' : 'followers_count',\n",
    "                         'user_friends_count' : 'friends_count',\n",
    "                         'place' : 'place_full_name',\n",
    "                         'user_screen_name' : 'screen_name',\n",
    "                         'id' : 'status_id',\n",
    "                         'user_verified' : 'verified',\n",
    "                         'user_created_at': 'account_created_at'\n",
    "    }, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvuMFAyBw4zB"
   },
   "outputs": [],
   "source": [
    "# Define function to correct data types and drop unnecessary columns\n",
    "def correct_columns_ieee(df):\n",
    "    # Set datetime columns as pandas datetime type\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['account_created_at'] = pd.to_datetime(df['account_created_at'])\n",
    "    \n",
    "    # Create boolean column for whether a tweet is a reply and drop all other reply columns\n",
    "    df['is_reply'] = df['in_reply_to_user_id'].notna()\n",
    "    df = df.drop(columns = ['in_reply_to_user_id','in_reply_to_status_id','in_reply_to_screen_name'])\n",
    "    \n",
    "    # Recreate the boolean is_retweet column\n",
    "    df['is_retweet'] = df['retweet_id'].notna()\n",
    "    \n",
    "    # Recreate country_code and place_type column\n",
    "    df['country_code'] = df['state'].map(lambda state: 'US' if state != 'unspecified' else 'unspecified')\n",
    "    df['place_type'] = 'unspecified'\n",
    "    \n",
    "    # Remove all unneeded columns\n",
    "    df = df.drop(columns = ['coordinates','media','possibly_sensitive','hashtags','urls','user_default_profile_image',\n",
    "                            'user_description','source','retweet_screen_name','user_favourites_count','user_listed_count'\n",
    "                            ,'user_location','user_name','user_screen_name.1','user_statuses_count','user_time_zone'\n",
    "                            ,'user_urls','retweet_id','tweet_url'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCweZ0EPw4zD"
   },
   "outputs": [],
   "source": [
    "# Define function to assign city names where identifiable\n",
    "def split_city(row, state_dict):\n",
    "    # Check whether the place name is split consistently with city/state names\n",
    "    if len(row['place_full_name'].split(', ')) != 2:\n",
    "        return 'unspecified'\n",
    "    else:\n",
    "        # Separate place name into components\n",
    "        first_split = row['place_full_name'].split(', ')[0]\n",
    "        second_split = row['place_full_name'].split(', ')[1]\n",
    "        \n",
    "        # Create masks for whether the first or second split are identifiable as state names or state codes\n",
    "        mask_second = (second_split not in state_dict.values() and second_split not in state_dict.keys())\n",
    "        mask_first = (first_split not in state_dict.values() and first_split not in state_dict.keys())\n",
    "        \n",
    "        # Assign city names based on place types and whether splits are identified as state names or codes\n",
    "        if row.get('place_type') == 'city':\n",
    "            return first_split\n",
    "        elif row.get('place_type') == 'neighborhood' and mask_second:\n",
    "            return second_split\n",
    "        elif row.get('place_type') == None and mask_first:\n",
    "            return first_split\n",
    "        else:\n",
    "            return 'unspecified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUTaQw_zw4zG"
   },
   "outputs": [],
   "source": [
    "# Define function to assign state codes where identifiable\n",
    "def split_state(row, state_dict):\n",
    "    # Set state as unspecified if it does not follow the standard location format\n",
    "    if len(row['place_full_name'].split(', ')) != 2:\n",
    "        return 'unspecified'\n",
    "    else:\n",
    "        # Separate place name into components\n",
    "        first_split = row['place_full_name'].split(', ')[0]\n",
    "        second_split = row['place_full_name'].split(', ')[1]\n",
    "        \n",
    "        # Create separate lists of state names and codes\n",
    "        state_codes = state_dict.values()\n",
    "        state_names = state_dict.keys()\n",
    "        \n",
    "        # Attempt to identify state codes and state names within first and second splits based on place type\n",
    "        if row.get('place_type') == None and (second_split in state_codes):\n",
    "            return second_split\n",
    "        elif row.get('place_type') == None and (second_split in state_names):\n",
    "            return state_dict[second_split]\n",
    "        elif row.get('place_type') == 'city' and second_split in state_codes:\n",
    "            return second_split\n",
    "        elif row.get('place_type') == 'neighborhood' and second_split in state_codes:\n",
    "            return second_split\n",
    "        elif row.get('place_type') == 'neighborhood' and second_split in state_names:\n",
    "            return state_dict[second_split]\n",
    "        elif (row.get('place_type') == 'admin' or row.get('place_type') == None) and first_split in state_names:\n",
    "            return state_dict[first_split]\n",
    "        else:\n",
    "            return 'unspecified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPBG0bUlw4zI"
   },
   "source": [
    "## Loop Through Files for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRwML4ikw4zI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the current combined US data CSV. If none exists, instantiate a dataframe for future joins\n",
    "try:\n",
    "    df_us = pd.read_csv('./data/combined_us.csv', index_col = 0)\n",
    "except:\n",
    "    df_us = pd.DataFrame()\n",
    "    \n",
    "# Loop through all files that have yet to be added to the combined \n",
    "for file in df_combined_data.loc[df_combined_data['added'] == 0, 'file']:\n",
    "    # Read file from CSV\n",
    "    df_file = pd.read_csv(f'./data/{file}')\n",
    "    \n",
    "    # Filter data down to only English tweets from the US with valid locations\n",
    "    # For IEEE formatted data, filter down only to valid place names\n",
    "    if file.find('IEEE') != -1:\n",
    "        df_file = correct_names_ieee(df_file)\n",
    "        df_file = df_file[df_file['place_full_name'].notna()]\n",
    "    else:\n",
    "        df_file = get_US_data(df_file)\n",
    "    \n",
    "    # Create city and state columns\n",
    "    df_file['city'] = df_file.apply(lambda row: split_city(row,us_state_abbrev), axis = 1)\n",
    "    df_file['state'] = df_file.apply(lambda row: split_state(row,us_state_abbrev), axis = 1)\n",
    "    \n",
    "    # Correct data types and missing columns    \n",
    "    if file.find('IEEE') != -1:\n",
    "        df_file = correct_columns_ieee(df_file)\n",
    "        df_file = get_US_data(df_file)\n",
    "    else:\n",
    "        df_file = correct_columns_kag(df_file)\n",
    "    \n",
    "    # Drop remaining nulls\n",
    "    # Commenting this step out for now to clarify what needs to be dropped\n",
    "#     df_file = df_file.dropna()\n",
    "    \n",
    "    # Add the data to the existing combined US data\n",
    "    df_us = df_file.append(df_us)\n",
    "    \n",
    "    # Update the combined data reference table to indicate the file has been read\n",
    "    df_combined_data.loc[df_combined_data['file'] == file, 'added'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate records\n",
    "df_us = df_us.drop_duplicates(subset = 'status_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0eZMCPIw4zK"
   },
   "source": [
    "## Update Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uB5hyO5Nw4zK"
   },
   "outputs": [],
   "source": [
    "# Update combined US data file\n",
    "df_us.to_csv('./Data/combined_us.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZqlziUcw4zN"
   },
   "outputs": [],
   "source": [
    "# Update reference file for included data\n",
    "df_combined_data.to_csv('./Data/combined_files.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "initial_data_cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
